<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>ML Stuff on </title>
    <link>https://KTS-o7.github.io/bear/ml/</link>
    <description>Recent content in ML Stuff on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <managingEditor>shentharkrishnatejaswi@gmail.com (Krishnatejaswi S)</managingEditor>
    <webMaster>shentharkrishnatejaswi@gmail.com (Krishnatejaswi S)</webMaster>
    <copyright>Krishnatejaswi S</copyright>
    <lastBuildDate>Thu, 08 Jan 2026 07:07:07 +0100</lastBuildDate>
    <atom:link href="https://KTS-o7.github.io/bear/ml/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Eigenvalues of a Matrix</title>
      <link>https://KTS-o7.github.io/bear/ml/eigen_vals/</link>
      <pubDate>Thu, 08 Jan 2026 07:07:07 +0100</pubDate><author>shentharkrishnatejaswi@gmail.com (Krishnatejaswi S)</author>
      <guid>https://KTS-o7.github.io/bear/ml/eigen_vals/</guid>
      <description>&lt;h2 id=&#34;why-eigenvalues-matter&#34;&gt;Why Eigenvalues Matter&lt;/h2&gt;&#xA;&lt;p&gt;Eigenvalues describe the directions along which a linear transformation stretches or compresses space. Behind every PCA dimension, every stability analysis, and every spectral decomposition lies the same question: &lt;em&gt;What scalars \(\lambda\) satisfy \(A\mathbf{v} = \lambda \mathbf{v}\) for some non-zero vector \(\mathbf{v}\)?&lt;/em&gt; Here we clarify the polynomial road map to those scalars and show how it generalizes from 2×2 matrices to higher dimensions, including why principal minors appear in the coefficients.&lt;/p&gt;</description>
      <content:encoded><![CDATA[<h2 id="why-eigenvalues-matter">Why Eigenvalues Matter</h2>
<p>Eigenvalues describe the directions along which a linear transformation stretches or compresses space. Behind every PCA dimension, every stability analysis, and every spectral decomposition lies the same question: <em>What scalars \(\lambda\) satisfy \(A\mathbf{v} = \lambda \mathbf{v}\) for some non-zero vector \(\mathbf{v}\)?</em> Here we clarify the polynomial road map to those scalars and show how it generalizes from 2×2 matrices to higher dimensions, including why principal minors appear in the coefficients.</p>
<h2 id="the-characteristic-equation">The Characteristic Equation</h2>
<p>For any square matrix \(A\), the eigenvalues are the roots of the characteristic polynomial:</p>
\[
\chi(\lambda) = \det(A - \lambda I) = 0.
\]<h3 id="22-refresher">2×2 refresher</h3>
<p>When \(A = \begin{bmatrix} a & b \\ c & d \end{bmatrix}\), the determinant expands to</p>
\[
\chi(\lambda) = \lambda^2 - (a+d)\lambda + (ad - bc).
\]<p>Hence the trace \((a+d)\) appears as the \(\lambda\)-coefficient and the determinant \(ad-bc\) is the constant term. Solving the quadratic gives the eigenvalues.</p>
<h2 id="worked-examples--and">Worked Examples: \(n=3\) and \(n=4\)</h2>
<h3>\(3 × 3\)</h3>
<p>Let</p>
\[
A = \begin{bmatrix}
a & b & c \\
d & e & f \\
g & h & i
\end{bmatrix}.
\]<p>Then \(A - \lambda I\) becomes</p>
\[
\begin{bmatrix}
a - \lambda & b & c \\
d & e - \lambda & f \\
g & h & i - \lambda
\end{bmatrix},
\]<p>
For a 3×3 matrix \( A = \begin{bmatrix} a & b & c \\ d & e & f \\ g & h & i \end{bmatrix} \), the characteristic polynomial expands as</p>
\[
\chi(\lambda) = \lambda^3 - \text{tr}(A)\,\lambda^2 + \sigma_2(A)\,\lambda - \det(A),
\]<p>where \(\sigma_2(A)\) is the sum of the 2×2 principal minors:</p>
\[
\sigma_2(A) = (ae - bd) + (ai - cg) + (ei - fh).
\]<p>Principal minors are the determinants of every square submatrix formed by selecting the same rows and columns. They appear because the expansion of \(\det(A - \lambda I)\) naturally produces sums over these subdeterminants—each coefficient gathers the contributions of principal minors of increasingly large size, which mathematically captures the pairwise, triple, etc., interactions between eigenvalues.</p>
<p>Notice:</p>
<ul>
<li>The \( \lambda^3 \) coefficient is 1 because the polynomial is monic.</li>
<li>The \(-\text{tr}(A)\) term mirrors the 2×2 case.</li>
<li>The constant term is \(-\det(A)\).</li>
<li>Intermediate coefficients aggregate principal minors.</li>
</ul>
<p><strong>Numeric example.</strong> For
</p>
\[
A = \begin{bmatrix}
1 & 2 & 3 \\
0 & 4 & 5 \\
0 & 0 & 6
\end{bmatrix},
\]<p>
the trace is 11, \(\sigma_2(A) = (1\cdot4 - 0\cdot2) + (1\cdot6 - 0\cdot3) + (4\cdot6 - 0\cdot5) = 4 + 6 + 24 = 34\), and the determinant is \(1\cdot4\cdot6 = 24\). The characteristic polynomial becomes \(\lambda^3 - 11\lambda^2 + 34\lambda - 24\), whose roots are the eigenvalues of \(A\).</p>
<h3>\(4 × 4\)</h3>
<p>For a 4×4 matrix, the pattern continues. Without loss of generality, \(A\) produces</p>
\[
\chi(\lambda) = \lambda^4 - \text{tr}(A)\,\lambda^3 + \sigma_2(A)\,\lambda^2 - \sigma_3(A)\,\lambda + \det(A),
\]<p>where:</p>
<ul>
<li>\(\sigma_2(A)\) sums all 2×2 principal minors,</li>
<li>\(\sigma_3(A)\) sums all 3×3 principal minors,</li>
<li>Signs alternate as dictated by the expansion of \(\det(A - \lambda I)\),</li>
<li>Trace and determinant remain the first and last invariants, and the intermediate \(\sigma_k\) tie into symmetric polynomials of eigenvalues.</li>
</ul>
<p>Each \(\sigma_k\) aggregates the determinants of the \(k \times k\) principal submatrices because the Leibniz expansion of the determinant iterates over all permutations of row/column pairs. Keeping track of these minors lets us express the characteristic polynomial coefficients without expanding every term manually, which is why they are a compact way to encode the invariant relationships between eigenvalues and the matrix entries.</p>
<p><strong>Numeric example.</strong> Take
</p>
\[
A = \begin{bmatrix}
2 & 0 & 1 & 0 \\
0 & 3 & 0 & 1 \\
0 & 0 & 4 & 0 \\
0 & 0 & 0 & 5
\end{bmatrix}.
\]<p>
The trace is 14, \(\sigma_2(A)\) collects the 2×2 minors (e.g., \(2\cdot3, 2\cdot4, 3\cdot4\) plus the shifts introduced by the 1s), and the determinant is \(2\cdot3\cdot4\cdot5 = 120\). The characteristic polynomial becomes \(\lambda^4 - 14\lambda^3 + \sigma_2(A)\lambda^2 - \sigma_3(A)\lambda + 120\), encoding the same invariants as before but for four eigenvalues.</p>
<p>These expressions remind us that the characteristic polynomial encodes global constraints: eigenvalues sum to the trace, pairwise products sum to \(\sigma_2(A)\), triple products to \(\sigma_3(A)\), and so on. Even though solving quartics or higher-degree polynomials analytically becomes impractical, numerical methods target the same invariants, so the conceptual road map stays intact.</p>
<h2 id="method-1-manual-calculation-educational">Method 1: Manual Calculation (Educational)</h2>





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="ln"> 1</span><span class="cl"><span class="kn">import</span> <span class="nn">math</span>
</span></span><span class="line"><span class="ln"> 2</span><span class="cl">
</span></span><span class="line"><span class="ln"> 3</span><span class="cl"><span class="k">def</span> <span class="nf">calculate_eigenvalues_2x2</span><span class="p">(</span><span class="n">A</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">float</span> <span class="o">|</span> <span class="nb">int</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]:</span>
</span></span><span class="line"><span class="ln"> 4</span><span class="cl">    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">A</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">!=</span> <span class="mi">2</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
</span></span><span class="line"><span class="ln"> 5</span><span class="cl">        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&#34;Input must be a 2x2 matrix.&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="ln"> 6</span><span class="cl">
</span></span><span class="line"><span class="ln"> 7</span><span class="cl">    <span class="n">trace</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">A</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="ln"> 8</span><span class="cl">    <span class="n">determinant</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">A</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">A</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">A</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="ln"> 9</span><span class="cl">
</span></span><span class="line"><span class="ln">10</span><span class="cl">    <span class="n">discriminant</span> <span class="o">=</span> <span class="n">trace</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">determinant</span>
</span></span><span class="line"><span class="ln">11</span><span class="cl">    <span class="k">if</span> <span class="n">discriminant</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="ln">12</span><span class="cl">        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&#34;Matrix has complex eigenvalues in this implementation.&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">13</span><span class="cl">
</span></span><span class="line"><span class="ln">14</span><span class="cl">    <span class="n">eigenvalue_1</span> <span class="o">=</span> <span class="p">(</span><span class="n">trace</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">discriminant</span><span class="p">))</span> <span class="o">/</span> <span class="mi">2</span>
</span></span><span class="line"><span class="ln">15</span><span class="cl">    <span class="n">eigenvalue_2</span> <span class="o">=</span> <span class="p">(</span><span class="n">trace</span> <span class="o">-</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">discriminant</span><span class="p">))</span> <span class="o">/</span> <span class="mi">2</span>
</span></span><span class="line"><span class="ln">16</span><span class="cl">
</span></span><span class="line"><span class="ln">17</span><span class="cl">    <span class="k">return</span> <span class="p">[</span><span class="n">eigenvalue_1</span><span class="p">,</span> <span class="n">eigenvalue_2</span><span class="p">]</span></span></span></code></pre></div><p>This hands-on route is excellent for reinforcing how the trace and determinant become coefficients of the characteristic polynomial and how complex eigenvalues arise when the discriminant is negative.</p>
<h2 id="method-2-numpy-for-the-general-case">Method 2: NumPy for the General Case</h2>





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="ln"> 1</span><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="ln"> 2</span><span class="cl">
</span></span><span class="line"><span class="ln"> 3</span><span class="cl"><span class="k">def</span> <span class="nf">calculate_eigenvalues_numpy</span><span class="p">(</span><span class="n">matrix</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">float</span> <span class="o">|</span> <span class="nb">int</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">complex</span><span class="p">]:</span>
</span></span><span class="line"><span class="ln"> 4</span><span class="cl">    <span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">matrix</span><span class="p">)</span>
</span></span><span class="line"><span class="ln"> 5</span><span class="cl">
</span></span><span class="line"><span class="ln"> 6</span><span class="cl">    <span class="k">if</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
</span></span><span class="line"><span class="ln"> 7</span><span class="cl">        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&#34;Input must be a square matrix.&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="ln"> 8</span><span class="cl">
</span></span><span class="line"><span class="ln"> 9</span><span class="cl">    <span class="n">eigenvalues</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigvals</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">10</span><span class="cl">    <span class="k">return</span> <span class="n">eigenvalues</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span></span></span></code></pre></div><p>NumPy hides the polynomial expansion but still solves \(\det(A - \lambda I) = 0\). It handles arbitrary square matrices, complex eigenvalues, and higher dimensions effortlessly.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Whether you expand a characteristic polynomial by hand for \(n=2\) or rely on NumPy for \(n>2\), the goal is the same: find the roots of \(\det(A - \lambda I)\). The trace and determinant continue to anchor the polynomial’s coefficients, while the intermediate terms encode sums of minors. Understanding this structure helps you make sense of the numerical results produced for large matrices and gives intuition for the algebra that underlies eigenvalue computation.</p>
]]></content:encoded>
    </item>
    <item>
      <title> Parameters in LLMs</title>
      <link>https://KTS-o7.github.io/bear/ml/params_llms/</link>
      <pubDate>Fri, 12 Sep 2025 17:23:41 +0530</pubDate><author>shentharkrishnatejaswi@gmail.com (Krishnatejaswi S)</author>
      <guid>https://KTS-o7.github.io/bear/ml/params_llms/</guid>
      <description>&lt;h1 id=&#34;generation-time-parameters-in-large-language-models&#34;&gt;Generation-Time Parameters in Large Language Models&lt;/h1&gt;&#xA;&lt;p&gt;I recently dove deep into how LLMs like GPT-3, GPT-4, and open-source models generate text, and I was fascinated by the sheer number of knobs and dials available to control their behavior. These aren&amp;rsquo;t just abstract concepts,they&amp;rsquo;re the tools that make the difference between getting a coherent essay or a rambling mess, between creative poetry or repetitive drivel.&lt;/p&gt;&#xA;&lt;p&gt;In this post, I&amp;rsquo;ll break down the key generation-time hyperparameters with practical examples, mathematical foundations, and real-world implications. We&amp;rsquo;ll go beyond the surface-level explanations and understand why each parameter matters and how to tune them effectively.&lt;/p&gt;</description>
      <content:encoded><![CDATA[<h1 id="generation-time-parameters-in-large-language-models">Generation-Time Parameters in Large Language Models</h1>
<p>I recently dove deep into how LLMs like GPT-3, GPT-4, and open-source models generate text, and I was fascinated by the sheer number of knobs and dials available to control their behavior. These aren&rsquo;t just abstract concepts,they&rsquo;re the tools that make the difference between getting a coherent essay or a rambling mess, between creative poetry or repetitive drivel.</p>
<p>In this post, I&rsquo;ll break down the key generation-time hyperparameters with practical examples, mathematical foundations, and real-world implications. We&rsquo;ll go beyond the surface-level explanations and understand why each parameter matters and how to tune them effectively.</p>
<h2 id="temperature-the-creativity-knob">Temperature: The Creativity Knob</h2>
<p>Temperature is arguably the most important parameter you&rsquo;ll encounter when using LLMs. It&rsquo;s like adjusting the &ldquo;personality&rdquo; of the model.</p>
<p><strong>What it does</strong>: Temperature scales the logits (raw prediction scores) before they&rsquo;re converted to probabilities via softmax. Mathematically:</p>
$$
P(w_i) = \frac{e^{z_i / T}}{\sum_j e^{z_j / T}}
$$<p>where <code>z_i</code> is the logit for token <code>i</code>, and <code>T</code> is temperature.</p>
<p><strong>Why it works</strong>: At lower temperatures (<code>T</code> &lt; 1), the probability distribution becomes sharper, high-probability tokens become even more likely, leading to more predictable, conservative outputs. At higher temperatures (<code>T</code> &gt; 1), the distribution flattens, giving low-probability tokens a better chance.</p>
<p><strong>Real-world example</strong>: When generating code, you want <code>T=0.1</code> for deterministic, correct syntax. For creative writing, <code>T=0.8-1.2</code> gives you that spark of originality. I&rsquo;ve seen <code>T=2.0</code> produce genuinely surprising and creative outputs, though sometimes bordering on incoherent.</p>
<p><strong>Typical range</strong>: <code>0.0</code> (completely deterministic) to <code>2.0</code> (highly random). Most applications use <code>0.7-1.0</code>.</p>
<h2 id="top-k-sampling-quality-control">Top-k Sampling: Quality Control</h2>
<p>Top-k is like telling the model, &ldquo;Only consider your top k most confident predictions.&rdquo;</p>
<p><strong>What it does</strong>: After computing all token probabilities, sort them and keep only the top k tokens, then renormalize their probabilities.</p>
<p><strong>Why it works</strong>: It prevents the model from picking extremely unlikely tokens that might derail the generation. For example, if the model is writing about &ldquo;coffee,&rdquo; top-k=50 ensures it doesn&rsquo;t suddenly switch to discussing quantum physics unless that was already in its top 50 predictions.</p>
<p><strong>Practical tip</strong>: For factual writing, use lower k (10-40) to stay on topic. For creative tasks, higher k (50+) allows more flexibility. I&rsquo;ve found k=1 essentially gives you greedy decoding,always pick the most likely token.</p>
<h2 id="top-p-nucleus-sampling-dynamic-quality-control">Top-p (Nucleus Sampling): Dynamic Quality Control</h2>
<p>Top-p takes top-k&rsquo;s idea but makes it adaptive.</p>
<p><strong>What it does</strong>: Instead of a fixed number of tokens, select the smallest set whose cumulative probability exceeds p. So if p=0.9, it might use 10 tokens one time and 50 tokens another time, depending on the probability distribution.</p>
<p><strong>Why it works</strong>: In scenarios where the model is very confident (sharp distribution), it uses fewer tokens. When uncertain (flat distribution), it considers more options. This is particularly useful for maintaining coherence while allowing creativity.</p>
<p><strong>Example</strong>: Writing a technical explanation,early tokens are predictable, so top-p might use 5-10 tokens. When getting creative, it expands to 20-30 tokens. I&rsquo;ve seen this outperform top-k in many creative writing tasks because it adapts to the model&rsquo;s confidence level.</p>
<p><strong>Typical range</strong>: 0.8-1.0. Values below 0.5 can be too restrictive.</p>
<h2 id="logit-bias-manual-steering">Logit Bias: Manual Steering</h2>
<p>This is the parameter for when you want to force the model&rsquo;s hand.</p>
<p><strong>What it does</strong>: Directly add or subtract values from specific token logits before sampling:</p>
$$
z_i' = z_i + b_i
$$<p><strong>Why it works</strong>: Want to ban certain words? Set their bias to -100. Want to encourage specific terminology? Add positive bias. I once used this to prevent an LLM from using certain brand names in generated content.</p>
<p><strong>Practical applications</strong>: Content moderation, style enforcement, or domain-specific vocabulary control. Particularly useful in enterprise settings where you need to avoid certain terms.</p>
<h2 id="repetition-penalty-fighting-the-echo-chamber">Repetition Penalty: Fighting the Echo Chamber</h2>
<p>Ever noticed how LLMs can get stuck repeating phrases? This parameter fights that.</p>
<p><strong>What it does</strong>: For tokens that have appeared recently, divide their logits by a penalty factor r &gt; 1:</p>
$$
z_i' = \frac{z_i}{r}
$$<p><strong>Why it works</strong>: It makes previously used tokens less attractive, encouraging the model to explore new vocabulary. Without this, models can get into loops like &ldquo;The best coffee is the best coffee is the best coffee&hellip;&rdquo;</p>
<p><strong>Real-world use</strong>: Essential for long-form content generation. I use values around 1.1-1.2 for most tasks, but for very creative work, sometimes 1.05 or lower to allow some intentional repetition for emphasis.</p>
<h2 id="stop-sequences-the-off-switch">Stop Sequences: The Off Switch</h2>
<p>Sometimes you need the model to know when to stop talking.</p>
<p><strong>What it does</strong>: Define specific strings that, when generated, halt the generation process.</p>
<p><strong>Why it works</strong>: Prevents runaway generation or ensures structured output. For example, when generating code, you might stop at the next function definition.</p>
<p><strong>Examples</strong>: <code>&quot;\n\n&quot;</code>, <code>&quot;END&quot;</code>, <code>&quot;&lt;/response&gt;&quot;</code>. I once used this to generate structured JSON by setting the stop sequence to <code>&quot;}&quot;</code>.</p>
<h2 id="max-tokens-setting-boundaries">Max Tokens: Setting Boundaries</h2>
<p>The word limit for your AI writer.</p>
<p><strong>What it does</strong>: Caps the total number of tokens generated in a single response.</p>
<p><strong>Why it works</strong>: Prevents excessive output and manages costs/compute time. In API contexts, this also affects billing.</p>
<p><strong>Considerations</strong>: Different models have different limits (GPT-4: 4096, Claude: 8192). Always set this lower than the model&rsquo;s maximum to leave headroom.</p>
<h2 id="frequency-penalty-discouraging-word-hoarders">Frequency Penalty: Discouraging Word Hoarders</h2>
<p>Similar to repetition penalty, but based on overall frequency in the generated text.</p>
<p><strong>What it does</strong>: Penalize tokens based on how often they&rsquo;ve appeared:</p>
$$
z_i' = z_i - \alpha \cdot \text{count}(i)
$$<p><strong>Why it works</strong>: Prevents overuse of common words. If &ldquo;the&rdquo; appears 50 times, it gets increasingly penalized.</p>
<p><strong>Use cases</strong>: Particularly effective for generating diverse content or when you want to avoid repetitive language patterns.</p>
<h2 id="presence-penalty-topic-freshness">Presence Penalty: Topic Freshness</h2>
<p>This encourages introducing new concepts rather than revisiting old ones.</p>
<p><strong>What it does</strong>: Penalize tokens that have appeared at all in the current generation:</p>
$$
z_i' = z_i - \beta \cdot \text{presence}(i)
$$<p>where presence(i) is 1 if the token appeared, 0 otherwise.</p>
<p><strong>Why it works</strong>: Promotes topic exploration. Without it, models can get stuck discussing the same ideas repeatedly.</p>
<p><strong>Difference from frequency penalty</strong>: Frequency counts occurrences, presence is binary. Use presence penalty for encouraging breadth of topics.</p>
<h2 id="num-keep-context-continuity">Num Keep: Context Continuity</h2>
<p>For long conversations or documents, this maintains coherence.</p>
<p><strong>What it does</strong>: When refreshing the context window, retain the last N tokens from the previous context.</p>
<p><strong>Why it works</strong>: Prevents abrupt topic changes in long-form generation. Imagine writing a novel,the model needs to remember what happened in the previous chapter.</p>
<p><strong>Typical range</strong>: 100-500 tokens, depending on the task complexity.</p>
<h2 id="seed-reproducibility-control">Seed: Reproducibility Control</h2>
<p>For when you need consistent outputs.</p>
<p><strong>What it does</strong>: Initializes the random number generator with a fixed value, ensuring identical outputs for identical inputs.</p>
<p><strong>Why it works</strong>: Crucial for testing, debugging, and applications requiring deterministic behavior. Without seeding, the same prompt can produce different outputs.</p>
<p><strong>Implementation note</strong>: Not all APIs expose seed control, but when available, it&rsquo;s invaluable for quality assurance.</p>
<h2 id="repeat-last-n-short-term-memory">Repeat Last N: Short-Term Memory</h2>
<p>A more targeted repetition control.</p>
<p><strong>What it does</strong>: Prevents tokens from the most recent N tokens from being selected.</p>
<p><strong>Why it works</strong>: Stops immediate repetition like &ldquo;I like coffee. I like coffee. I like coffee.&rdquo;</p>
<p><strong>Typical range</strong>: 1-10 tokens. Higher values can prevent natural repetition that&rsquo;s actually desirable.</p>
<h2 id="mirostat-adaptive-temperature">Mirostat: Adaptive Temperature</h2>
<p>This is for the advanced users who want the model to maintain consistent &ldquo;surprise&rdquo; levels.</p>
<p><strong>What it does</strong>: Dynamically adjusts temperature to target a specific perplexity level using parameters τ (target perplexity) and η (adjustment rate).</p>
<p><strong>Why it works</strong>: Maintains consistent creativity levels throughout generation. Traditional temperature can lead to increasingly erratic outputs as generation progresses.</p>
<p><strong>Typical range</strong>: τ: 20-100, η: 0.05-0.2. Requires experimentation to find good values for your use case.</p>
<h2 id="putting-it-all-together">Putting It All Together</h2>
<p>These parameters don&rsquo;t exist in isolation, they interact in complex ways. For most applications, you&rsquo;ll primarily tune temperature, top-p, and repetition penalties. But understanding all of them gives you the full toolkit for controlling LLM behavior.</p>
<p>Remember: there&rsquo;s no &ldquo;perfect&rdquo; set of parameters. The best settings depend on your specific use case, model, and desired output characteristics. Experimentation and iteration are key.</p>
<p>What parameter have you found most useful in your LLM work? I&rsquo;d love to hear your thoughts!</p>
<p>ʕ •ᴥ•ʔ</p>
]]></content:encoded>
    </item>
    <item>
      <title>Matrix-Vector Dot Product</title>
      <link>https://KTS-o7.github.io/bear/ml/matrix_vec_dot/</link>
      <pubDate>Sat, 06 Sep 2025 07:07:07 +0100</pubDate><author>shentharkrishnatejaswi@gmail.com (Krishnatejaswi S)</author>
      <guid>https://KTS-o7.github.io/bear/ml/matrix_vec_dot/</guid>
      <description>&lt;h2 id=&#34;matrix-vector-dot-product&#34;&gt;Matrix-Vector Dot Product&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.deep-ml.com/problems/1&#34;&gt;Question 1 From deep-ml.com&lt;/a&gt;&#xA;Essentially a dot product of a matrix and a vector an happen only if the number of columns in the matrix is equal to the number of elements in the vector.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;That is if A[m,n] and B[n], then A*B is defined.&lt;/p&gt;&#xA;&lt;p&gt;So in this case, the matrix is 2x3 and the vector is 3x1, so the dot product is defined.&lt;/p&gt;&#xA;&lt;p&gt;The result is a 2x1 matrix.&lt;/p&gt;</description>
      <content:encoded><![CDATA[<h2 id="matrix-vector-dot-product">Matrix-Vector Dot Product</h2>
<ol>
<li><a href="https://www.deep-ml.com/problems/1">Question 1 From deep-ml.com</a>
Essentially a dot product of a matrix and a vector an happen only if the number of columns in the matrix is equal to the number of elements in the vector.</li>
</ol>
<p>That is if A[m,n] and B[n], then A*B is defined.</p>
<p>So in this case, the matrix is 2x3 and the vector is 3x1, so the dot product is defined.</p>
<p>The result is a 2x1 matrix.</p>
<p>The result is:</p>
$$
\begin{bmatrix}
1 & 2 & 3 \\
4 & 5 & 6 \\
\end{bmatrix}
\cdot
\begin{bmatrix}
1 \\
2 \\
3 \\
\end{bmatrix}
\equiv
\begin{bmatrix}
1 \cdot 1 + 2 \cdot 2 + 3 \cdot 3 \\
4 \cdot 1 + 5 \cdot 2 + 6 \cdot 3 \\
\end{bmatrix}
\equiv
\begin{bmatrix}
14 \\
32 \\
\end{bmatrix}
$$<h2 id="solution">Solution</h2>





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="ln"> 1</span><span class="cl"><span class="k">def</span> <span class="nf">matrix_dot_vector</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="o">|</span><span class="nb">float</span><span class="p">]],</span> <span class="n">b</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="o">|</span><span class="nb">float</span><span class="p">])</span>
</span></span><span class="line"><span class="ln"> 2</span><span class="cl"> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="o">|</span><span class="nb">float</span><span class="p">]:</span>
</span></span><span class="line"><span class="ln"> 3</span><span class="cl">	<span class="c1"># Return a list where each element is the dot product of a row of &#39;a&#39; with &#39;b&#39;.</span>
</span></span><span class="line"><span class="ln"> 4</span><span class="cl">	<span class="c1"># If the number of columns in &#39;a&#39; does not match the length of &#39;b&#39;, return -1.</span>
</span></span><span class="line"><span class="ln"> 5</span><span class="cl">    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">b</span><span class="p">):</span>
</span></span><span class="line"><span class="ln"> 6</span><span class="cl">        <span class="k">return</span> <span class="o">-</span><span class="mi">1</span>
</span></span><span class="line"><span class="ln"> 7</span><span class="cl">    <span class="n">c</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="ln"> 8</span><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">a</span><span class="p">)):</span>
</span></span><span class="line"><span class="ln"> 9</span><span class="cl">        <span class="nb">sum</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="ln">10</span><span class="cl">        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">a</span><span class="p">)):</span>
</span></span><span class="line"><span class="ln">11</span><span class="cl">            <span class="nb">sum</span> <span class="o">+=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span><span class="o">*</span><span class="n">b</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
</span></span><span class="line"><span class="ln">12</span><span class="cl">
</span></span><span class="line"><span class="ln">13</span><span class="cl">        <span class="n">c</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">sum</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">14</span><span class="cl">	<span class="k">return</span> <span class="n">c</span></span></span></code></pre></div><hr>
]]></content:encoded>
    </item>
  </channel>
</rss>
